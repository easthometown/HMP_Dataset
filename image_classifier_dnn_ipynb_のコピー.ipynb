{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image-classifier-dnn.ipynb のコピー",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/easthometown/HMP_Dataset/blob/coursera/image_classifier_dnn_ipynb_%E3%81%AE%E3%82%B3%E3%83%94%E3%83%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC2XM9ueyLhu"
      },
      "source": [
        "# Image Classification: Dense Neural Network\n",
        "\n",
        "[![Open In Colab <](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ShawnHymel/computer-vision-with-embedded-machine-learning/blob/master/1.2.3%20-%20Training%20an%20Image%20Classifier%20with%20Keras/image_classifier_dnn.ipynb)\n",
        "\n",
        "Run this notebook to train a dense neural network (DNN) on your own image dataset.\n",
        "\n",
        "Based on: https://colab.research.google.com/github/AviatorMoser/keras-mnist-tutorial/blob/master/MNIST%20in%20Keras.ipynb#scrollTo=rFFriuVPwhAm\n",
        "\n",
        "(I recommend working through the example above, as it has a lot of good descriptions on doing classification with images)\n",
        "\n",
        "Create a folder named \"dataset\" in the /content directory and upload your images there. The images should be divided into their respective classes, where each class has its own folder with the name of the class. For example:\n",
        "\n",
        "<pre>\n",
        "/content\n",
        "    |- dataset\n",
        "        |- background\n",
        "        |- capacitor\n",
        "        |- diode\n",
        "        |- led\n",
        "        |- resistor\n",
        "</pre>\n",
        "\n",
        "Author: EdgeImpulse, Inc.<br>\n",
        "Date: June 5, 2021<br>\n",
        "License: [Apache-2.0](apache.org/licenses/LICENSE-2.0)<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 新しいセクション"
      ],
      "metadata": {
        "id": "Dd964NM0UjRm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhHG2__GyJjs"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import os\n",
        "import PIL\n",
        "\n",
        "from keras.utils import np_utils                          # tools for creating one-hot encoding\n",
        "from keras.models import Sequential                       # Type of model we wish to use\n",
        "from keras.layers.core import Dense, Dropout, Activation  # Types of layers we wish to use\n",
        "\n",
        "from skimage.transform import resize                      # Used to scale/resize image arrays\n",
        "\n",
        "from sklearn.metrics import confusion_matrix              # Used to quickly make confusion matrix"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wu2l_Lm0yhtJ"
      },
      "source": [
        "### Settings\n",
        "\n",
        "# Location of dataset\n",
        "DATASET_PATH = \"/content/dataset\"\n",
        "\n",
        "# Desired resolution of images\n",
        "TARGET_WIDTH = 28\n",
        "TARGET_HEIGHT = 28\n",
        "\n",
        "# Invert image (dark backgrounds can sometimes improve accuracy)\n",
        "INVERT = False\n",
        "\n",
        "# Set aside 20% for validation and 20% for test\n",
        "VAL_RATIO = 0.2\n",
        "TEST_RATIO = 0.2\n",
        "\n",
        "# You are welcome to change the seed to try a different validation set split\n",
        "random.seed(42)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXcJLxcv0yOT",
        "outputId": "5047b964-7cb7-4ed7-e129-e391adae277f"
      },
      "source": [
        "### Load images as Numpy arrays\n",
        "\n",
        "# We want to record the labels and assign a ground truth label as a number to each sample\n",
        "labels = []\n",
        "y_all = []    # Lowercase 'y' - 1D vector of the ground labels (n)\n",
        "X_all = []    # Uppercase 'X' - 3D array of all image samples (n x width x height)\n",
        "\n",
        "# Find the directories in the dataset folder (skip the Jupyter Notebook checkpoints hidden folder)\n",
        "for label in os.listdir(DATASET_PATH):\n",
        "  class_dir = os.path.join(DATASET_PATH, label)\n",
        "  if os.path.isdir(class_dir) and label != \".ipynb_checkpoints\":\n",
        "\n",
        "    # Add the name of the folder to our labels list\n",
        "    labels.append(label)\n",
        "\n",
        "    # Go through each image in the folder\n",
        "    for i, file in enumerate(os.listdir(class_dir)):\n",
        "\n",
        "      # Skip the Jupyter Notebook checkpoints folder that sometimes gets added\n",
        "      if file != \".ipynb_checkpoints\":\n",
        "\n",
        "        # Open image and convert to grayscale\n",
        "        file_path = os.path.join(class_dir, file)\n",
        "        img = PIL.Image.open(file_path).convert('L')\n",
        "\n",
        "        # Convert the image to a Numpy array, optionally invern, and append to X\n",
        "        img_array = np.asarray(img)\n",
        "        if INVERT:\n",
        "          img_array = 255 - img_array\n",
        "        X_all.append(img_array)\n",
        "\n",
        "        # Add label to the y array\n",
        "        y_all.append(label)\n",
        "\n",
        "    # Show how many images we loaded\n",
        "    print(\"Added\", str(i + 1), \"images from\", label)\n",
        "\n",
        "# Calculate total number of samples\n",
        "num_samples = len(X_all)\n",
        "\n",
        "# Sort the labels list by alphabetical order\n",
        "labels = sorted(labels)\n",
        "\n",
        "# Print out labels and number of samples\n",
        "print(labels)\n",
        "print(\"Number of samples:\", num_samples)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 54 images from image\n",
            "['image']\n",
            "Number of samples: 54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ux1q5Z_nFq0",
        "outputId": "5ffb5908-4655-4b15-d1aa-a0cde8d761d6"
      },
      "source": [
        "### Convert labels to numbers\n",
        "\n",
        "# Show the labels before the conversion\n",
        "print(\"Before:\", y_all)\n",
        "\n",
        "# Convert each label to its index in the labels\n",
        "y_out = []\n",
        "for i, label in enumerate(y_all):\n",
        "  y_out.append(labels.index(label))\n",
        "y_all = y_out\n",
        "\n",
        "# Show the labels after the conversion\n",
        "print(\"After:\", y_all)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before: ['image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image', 'image']\n",
            "After: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJrS0Oz31NkI",
        "outputId": "75a54b77-0aab-4d34-c06e-00398f7c6570"
      },
      "source": [
        "### Shuffle samples and labels together, divide into test, validation, and training sets\n",
        "\n",
        "# Shuffle samples and associated labels together\n",
        "X_y = list(zip(X_all, y_all))\n",
        "random.shuffle(X_y)\n",
        "X_all, y_all = zip(*X_y)\n",
        "\n",
        "# Calculate number of validation and test samples to put aside (round down)\n",
        "num_samples_test = int(TEST_RATIO * num_samples)\n",
        "num_samples_val = int(VAL_RATIO * num_samples)\n",
        "\n",
        "# The first `num_samples_test` samples of the shuffled list becomes the test set\n",
        "X_test = X_all[:num_samples_test]\n",
        "y_test = y_all[:num_samples_test]\n",
        "\n",
        "# The next `num_samples_val` samples of the shuffled list becomes the validation set\n",
        "X_val = X_all[num_samples_test:(num_samples_test + num_samples_val)]\n",
        "y_val = y_all[num_samples_test:(num_samples_test + num_samples_val)]\n",
        "\n",
        "# The remaining samples become the training set\n",
        "X_train = X_all[(num_samples_test + num_samples_val):]\n",
        "y_train = y_all[(num_samples_test + num_samples_val):]\n",
        "\n",
        "# Remember the number of samples in the test set\n",
        "num_samples_train = len(X_train)\n",
        "\n",
        "# Print out the number of test, validation, and training samples\n",
        "print(\"Number of test samples:\", num_samples_test)\n",
        "print(\"Number of validation samples:\", num_samples_val)\n",
        "print(\"Number of training samples:\", num_samples_train)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test samples: 10\n",
            "Number of validation samples: 10\n",
            "Number of training samples: 34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6FwDtw_DazYG",
        "outputId": "fb6da76d-4692-4898-8c63-209eb1a75c18"
      },
      "source": [
        "### View one of the training samples\n",
        "\n",
        "# Chose which sample you want to view\n",
        "idx = 0\n",
        "\n",
        "# Print out label (numbe and string) and part of the array\n",
        "print(\"Label: \" + str(y_train[idx]) + \" (\" + labels[y_train[idx]] + \")\")\n",
        "print(X_train[idx])\n",
        "\n",
        "# Display image from array\n",
        "plt.imshow(X_train[idx], cmap='gray', vmin=0, vmax=255)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 0 (image)\n",
            "[[0.30280112 0.29803922 0.28627451 0.2745098  0.29495798 0.38515406\n",
            "  0.49037615 0.49411765 0.41176471 0.38431373 0.25490196 0.22352941\n",
            "  0.30196078 0.34509804 0.27058824 0.22745098 0.21568627 0.20084034\n",
            "  0.19215686 0.18431373 0.17647059 0.17254902 0.18823529 0.23137255\n",
            "  0.33963585 0.39215686 0.41176471 0.42352941]\n",
            " [0.28627451 0.28005202 0.2745098  0.26008403 0.25490196 0.25882353\n",
            "  0.31232493 0.40842337 0.42352941 0.39215686 0.29551821 0.29551821\n",
            "  0.3372549  0.36470588 0.2745098  0.18683473 0.1789916  0.17114846\n",
            "  0.16722689 0.16078431 0.15294118 0.14901961 0.15686275 0.18431373\n",
            "  0.24173669 0.33595438 0.4        0.41960784]\n",
            " [0.27164866 0.26536615 0.25882353 0.24705882 0.23557423 0.23137255\n",
            "  0.22745098 0.2512605  0.33753501 0.47030812 0.48389356 0.46610644\n",
            "  0.38805522 0.36470588 0.27142857 0.15686275 0.14509804 0.14509804\n",
            "  0.14117647 0.1372549  0.12941176 0.1254902  0.1254902  0.14901961\n",
            "  0.18641457 0.24315726 0.34527811 0.40784314]\n",
            " [0.27058824 0.2627451  0.2512605  0.23921569 0.22745098 0.21568627\n",
            "  0.21176471 0.21176471 0.28767507 0.46848739 0.65588235 0.6605042\n",
            "  0.43277311 0.36778711 0.2745098  0.15294118 0.1254902  0.12352941\n",
            "  0.12156863 0.11764706 0.11372549 0.11372549 0.10980392 0.12156863\n",
            "  0.16372549 0.19607843 0.25028011 0.34551821]\n",
            " [0.29803922 0.26694678 0.25490196 0.23921569 0.22352941 0.21176471\n",
            "  0.20392157 0.20392157 0.2837535  0.3487395  0.54677871 0.68653461\n",
            "  0.49971989 0.36470588 0.27142857 0.14901961 0.12156863 0.11372549\n",
            "  0.11372549 0.11372549 0.11372549 0.10980392 0.10980392 0.11764706\n",
            "  0.15294118 0.17254902 0.19989996 0.24285714]\n",
            " [0.42605042 0.42857143 0.44835934 0.3429972  0.24565826 0.21568627\n",
            "  0.20392157 0.20784314 0.23921569 0.25490196 0.50392157 0.76862745\n",
            "  0.58319328 0.37170868 0.27142857 0.14901961 0.11764706 0.10980392\n",
            "  0.11372549 0.11372549 0.11372549 0.10980392 0.10980392 0.11736695\n",
            "  0.15686275 0.16470588 0.17647059 0.19607843]\n",
            " [0.62829132 0.66776711 0.77647059 0.67731092 0.47989196 0.29133653\n",
            "  0.21176471 0.22352941 0.22745098 0.25568227 0.50042017 0.74117647\n",
            "  0.56248499 0.36778711 0.267507   0.14761905 0.11764706 0.10980392\n",
            "  0.11064426 0.11372549 0.11372549 0.10980392 0.10588235 0.11372549\n",
            "  0.15686275 0.16470588 0.16470588 0.17563025]\n",
            " [0.67909164 0.55716287 0.81170468 0.76428571 0.7215086  0.4907563\n",
            "  0.2697479  0.24705882 0.23137255 0.27843137 0.55098039 0.76840736\n",
            "  0.55826331 0.35686275 0.2590036  0.14509804 0.11764706 0.10980392\n",
            "  0.11372549 0.11372549 0.11372549 0.11680672 0.11680672 0.1254902\n",
            "  0.16078431 0.17254902 0.16862745 0.16862745]\n",
            " [0.64313725 0.49803922 0.68739496 0.70252101 0.65742297 0.52156863\n",
            "  0.29411765 0.24705882 0.24935974 0.30718287 0.51176471 0.69411765\n",
            "  0.51742697 0.34117647 0.24789916 0.14117647 0.11372549 0.10980392\n",
            "  0.12296919 0.1372549  0.14509804 0.14509804 0.14509804 0.14901961\n",
            "  0.16862745 0.17647059 0.17647059 0.17254902]\n",
            " [0.71848739 0.4822529  0.46692677 0.59243697 0.55322129 0.36088435\n",
            "  0.26296519 0.23529412 0.2627451  0.39187675 0.61358543 0.71764706\n",
            "  0.5269908  0.35602241 0.267507   0.14901961 0.11764706 0.10980392\n",
            "  0.13333333 0.14901961 0.15294118 0.14509804 0.14117647 0.14901961\n",
            "  0.16862745 0.18039216 0.19607843 0.19215686]\n",
            " [0.6372549  0.49663866 0.39803922 0.54313725 0.43305322 0.28235294\n",
            "  0.2627451  0.23921569 0.29089636 0.53501401 0.68823529 0.69803922\n",
            "  0.52366947 0.35294118 0.27142857 0.15294118 0.12156863 0.10980392\n",
            "  0.12969188 0.14901961 0.14901961 0.14117647 0.13333333 0.13333333\n",
            "  0.16470588 0.16470588 0.15686275 0.14901961]\n",
            " [0.55266106 0.39047619 0.34455782 0.517507   0.38377351 0.27040816\n",
            "  0.27058824 0.31008403 0.46442577 0.64311725 0.69411765 0.68233293\n",
            "  0.51484594 0.34509804 0.26666667 0.15546218 0.1254902  0.11372549\n",
            "  0.12156863 0.13333333 0.14145658 0.14901961 0.14929972 0.15294118\n",
            "  0.17647059 0.16078431 0.14117647 0.14117647]\n",
            " [0.43723489 0.32016807 0.32913165 0.46414566 0.33305322 0.28235294\n",
            "  0.30980392 0.36470588 0.50336134 0.61018407 0.63921569 0.63921569\n",
            "  0.49551821 0.34117647 0.26358543 0.15686275 0.12941176 0.12156863\n",
            "  0.13977591 0.16862745 0.19215686 0.18039216 0.17647059 0.17254902\n",
            "  0.18039216 0.16834734 0.14117647 0.14509804]\n",
            " [0.41260504 0.34985994 0.40470188 0.41960784 0.29019608 0.32603041\n",
            "  0.38123249 0.37581032 0.51120448 0.60308123 0.62352941 0.63053221\n",
            "  0.47983193 0.3372549  0.2627451  0.16078431 0.13333333 0.12941176\n",
            "  0.15714286 0.19215686 0.20392157 0.18189276 0.18431373 0.17647059\n",
            "  0.17254902 0.16470588 0.14201681 0.14509804]\n",
            " [0.40802321 0.37647059 0.42038816 0.37254902 0.31680672 0.37845138\n",
            "  0.40784314 0.36862745 0.45128051 0.52234894 0.57885154 0.60756303\n",
            "  0.44705882 0.32941176 0.26358543 0.16470588 0.1372549  0.13333333\n",
            "  0.16106443 0.19215686 0.20784314 0.18823529 0.18823529 0.18039216\n",
            "  0.17647059 0.172489   0.15602241 0.16078431]\n",
            " [0.39215686 0.35938375 0.39215686 0.36862745 0.37787115 0.39215686\n",
            "  0.40392157 0.35968387 0.42442977 0.45882353 0.43263305 0.55294118\n",
            "  0.43669468 0.332493   0.267507   0.16470588 0.1372549  0.1372549\n",
            "  0.16470588 0.18823529 0.19215686 0.18039216 0.18431373 0.18039216\n",
            "  0.18039216 0.2        0.21960784 0.22492997]\n",
            " [0.38039216 0.33333333 0.36862745 0.39215686 0.38039216 0.38823529\n",
            "  0.38823529 0.35294118 0.45080032 0.51764706 0.38823529 0.44367747\n",
            "  0.40784314 0.32156863 0.25490196 0.15938375 0.1372549  0.1372549\n",
            "  0.16470588 0.18431373 0.18823529 0.16862745 0.17254902 0.17647059\n",
            "  0.18431373 0.20784314 0.21176471 0.21176471]\n",
            " [0.38431373 0.32156863 0.35686275 0.37647059 0.37647059 0.38431373\n",
            "  0.38039216 0.34901961 0.43529412 0.41372549 0.36470588 0.45308123\n",
            "  0.42422969 0.26862745 0.22745098 0.16470588 0.14117647 0.14117647\n",
            "  0.16470588 0.18683473 0.23921569 0.19607843 0.18431373 0.16862745\n",
            "  0.18137255 0.20784314 0.20784314 0.20784314]\n",
            " [0.45910364 0.38851541 0.36862745 0.38039216 0.37254902 0.37226891\n",
            "  0.36862745 0.3372549  0.37254902 0.31372549 0.34915966 0.4630052\n",
            "  0.42334934 0.1832533  0.16862745 0.16078431 0.14117647 0.14117647\n",
            "  0.16078431 0.18683473 0.24313725 0.20392157 0.18291317 0.15658263\n",
            "  0.1802521  0.21176471 0.21568627 0.21176471]\n",
            " [0.52941176 0.42997199 0.36862745 0.40252101 0.36834734 0.34257703\n",
            "  0.34117647 0.31764706 0.37647059 0.36862745 0.4012605  0.36470588\n",
            "  0.28677471 0.16276511 0.18039216 0.15686275 0.1372549  0.1372549\n",
            "  0.16470588 0.2442577  0.31148459 0.2535014  0.20734294 0.14117647\n",
            "  0.17254902 0.21568627 0.21960784 0.21960784]\n",
            " [0.49327731 0.40362145 0.35686275 0.39915966 0.35580232 0.32156863\n",
            "  0.31372549 0.30980392 0.39801921 0.39221689 0.41960784 0.33697479\n",
            "  0.28235294 0.22745098 0.23529412 0.16916767 0.13333333 0.1372549\n",
            "  0.19321729 0.39497799 0.45098039 0.38581433 0.28517407 0.14117647\n",
            "  0.16470588 0.21176471 0.21960784 0.21960784]\n",
            " [0.36470588 0.33333333 0.34117647 0.35686275 0.32134854 0.29605842\n",
            "  0.28319328 0.31306523 0.48737495 0.45804322 0.45098039 0.36834734\n",
            "  0.21370548 0.1962585  0.29393758 0.18683473 0.12156863 0.1372549\n",
            "  0.21988796 0.42997199 0.47058824 0.41960784 0.302501   0.14929972\n",
            "  0.16862745 0.21960784 0.22745098 0.22352941]\n",
            " [0.3372549  0.29803922 0.31390556 0.29663866 0.26526611 0.24011605\n",
            "  0.22352941 0.28935574 0.50980392 0.50990396 0.48235294 0.34901961\n",
            "  0.18823529 0.16162465 0.29719888 0.17737095 0.11372549 0.1372549\n",
            "  0.2394958  0.40784314 0.38683473 0.30338135 0.24565826 0.14901961\n",
            "  0.16666667 0.21960784 0.22745098 0.22745098]\n",
            " [0.32941176 0.23921569 0.23893557 0.23921569 0.23893557 0.23921569\n",
            "  0.239996   0.3215086  0.51904762 0.56444578 0.52156863 0.36834734\n",
            "  0.19355742 0.14985994 0.29803922 0.21166467 0.1254902  0.1372549\n",
            "  0.25518207 0.29177671 0.18067227 0.15686275 0.19607843 0.14901961\n",
            "  0.16470588 0.21568627 0.22352941 0.22352941]\n",
            " [0.44117647 0.40196078 0.39803922 0.39117647 0.39019608 0.37843137\n",
            "  0.3627451  0.42156863 0.5372549  0.58039216 0.5254902  0.37647059\n",
            "  0.21176471 0.14509804 0.2697479  0.27969188 0.17857143 0.14901961\n",
            "  0.28641457 0.25630252 0.14901961 0.14901961 0.21176471 0.16484594\n",
            "  0.17058824 0.21176471 0.22352941 0.22352941]\n",
            " [0.43507403 0.36432573 0.34089636 0.33305322 0.32521008 0.30952381\n",
            "  0.33305322 0.46666667 0.49019608 0.49411765 0.47058824 0.3884954\n",
            "  0.23687475 0.14509804 0.30896359 0.42913165 0.36552621 0.18039216\n",
            "  0.30196078 0.25228091 0.14901961 0.15294118 0.2210084  0.1767507\n",
            "  0.18039216 0.21568627 0.22352941 0.22745098]\n",
            " [0.38853541 0.28235294 0.25098039 0.25098039 0.25882353 0.23137255\n",
            "  0.30672269 0.52408964 0.54369748 0.5280112  0.48095238 0.42735094\n",
            "  0.24845938 0.14985994 0.3442577  0.49019608 0.4512605  0.21960784\n",
            "  0.26676671 0.20622249 0.14117647 0.14901961 0.21176471 0.17647059\n",
            "  0.18039216 0.22352941 0.23529412 0.23921569]\n",
            " [0.36862745 0.28235294 0.25490196 0.25882353 0.2745098  0.23529412\n",
            "  0.33809524 0.60308123 0.74201681 0.71064426 0.59061625 0.43501401\n",
            "  0.21286515 0.15312125 0.35602241 0.45044018 0.43921569 0.2337535\n",
            "  0.18431373 0.15294118 0.1372549  0.15378151 0.17647059 0.16862745\n",
            "  0.18039216 0.22352941 0.23529412 0.23921569]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f0991f3f910>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eVgRZnHc6_y"
      },
      "source": [
        "### Function to resize list of images\n",
        "def resize_images(images, width, height, anti_aliasing=True):\n",
        "  \"\"\"\n",
        "  Prove a list of Numpy arrays (in images parameter) to have them all resized to desired height and\n",
        "  width. Returns the list of newly resized image arrays.\n",
        "\n",
        "  NOTE: skimage resize returns *normalized* image arrays (values between 0..1)\n",
        "  \"\"\"\n",
        "  X_out = []\n",
        "  for i, img in enumerate(images):\n",
        "    X_out.append(resize(img, (height, width), anti_aliasing=anti_aliasing))\n",
        "  return X_out"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNpuq5Lxa9Ck"
      },
      "source": [
        "### Scale/crop images\n",
        "\n",
        "# Resize (scale) all images in the training set\n",
        "X_train = resize_images(X_train, TARGET_WIDTH, TARGET_HEIGHT)\n",
        "\n",
        "# Resize (scale) all images in the validation set\n",
        "X_val = resize_images(X_val, TARGET_WIDTH, TARGET_HEIGHT)\n",
        "\n",
        "# Resize (scale) all images in the test set\n",
        "X_test = resize_images(X_test, TARGET_WIDTH, TARGET_HEIGHT)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "tL9W4Pw1cVon",
        "outputId": "6747a6e9-be1d-4b8c-b54c-0807154eb911"
      },
      "source": [
        "### View training sample again (after they all have been scaled)\n",
        "\n",
        "# Chose which sample you want to view\n",
        "idx = 0\n",
        "\n",
        "# Print out label (numbe and string) and part of the array\n",
        "print(\"Label: \" + str(y_train[idx]) + \" (\" + labels[y_train[idx]] + \")\")\n",
        "print(\"First row:\", X_train[idx][:1,:])\n",
        "\n",
        "# Display image from array (note that images have been normalized)\n",
        "plt.imshow(X_train[idx], cmap='gray', vmin=0, vmax=1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 0 (image)\n",
            "First row: [[0.30280112 0.29803922 0.28627451 0.2745098  0.29495798 0.38515406\n",
            "  0.49037615 0.49411765 0.41176471 0.38431373 0.25490196 0.22352941\n",
            "  0.30196078 0.34509804 0.27058824 0.22745098 0.21568627 0.20084034\n",
            "  0.19215686 0.18431373 0.17647059 0.17254902 0.18823529 0.23137255\n",
            "  0.33963585 0.39215686 0.41176471 0.42352941]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f0991ec6650>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E13XUha044fM",
        "outputId": "52d859e7-4a82-4182-8376-a9f3339e9db7"
      },
      "source": [
        "### Convert list of samples and labels into Numpy arrays\n",
        "\n",
        "# Training set\n",
        "X_train = np.asarray(X_train)\n",
        "y_train = np.asarray(y_train)\n",
        "\n",
        "# Validation set\n",
        "X_val = np.asarray(X_val)\n",
        "y_val = np.asarray(y_val)\n",
        "\n",
        "# Test set\n",
        "X_test = np.asarray(X_test)\n",
        "y_test = np.asarray(y_test)\n",
        "\n",
        "# Print out the new Numpy array shapes (always a good idea to check the shapes!)\n",
        "print(\"Training X:\", X_train.shape)\n",
        "print(\"Training y:\", y_train.shape)\n",
        "print(\"Validation X:\", X_val.shape)\n",
        "print(\"Validation y:\", y_val.shape)\n",
        "print(\"Test X:\", X_test.shape)\n",
        "print(\"Test y:\", y_test.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training X: (34, 28, 28)\n",
            "Training y: (34,)\n",
            "Validation X: (10, 28, 28)\n",
            "Validation y: (10,)\n",
            "Test X: (10, 28, 28)\n",
            "Test y: (10,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bf-p0MnJ9csm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2f4acf6-9464-4c40-f4db-1498050b449c"
      },
      "source": [
        "### Flatten each image to a 1D vector (DNN requires 1D input)\n",
        "\n",
        "# Compute length of 1D array that we will flatten each image to\n",
        "len_vector = TARGET_WIDTH * TARGET_WIDTH\n",
        "\n",
        "# Flatten matricies to vectors\n",
        "X_train = X_train.reshape(num_samples_train, len_vector)\n",
        "X_val = X_val.reshape(num_samples_val, len_vector)\n",
        "X_test = X_test.reshape(num_samples_test, len_vector)\n",
        "\n",
        "# Determine the input shape for our Keras model (must be tuple)\n",
        "input_shape = (X_train.shape[1],)\n",
        "\n",
        "# Print out shapes\n",
        "print(\"X train:\", X_train.shape)\n",
        "print(\"y train:\", y_train.shape)\n",
        "print(\"X val:\", X_val.shape)\n",
        "print(\"y val:\", y_val.shape)\n",
        "print(\"X test:\", X_test.shape)\n",
        "print(\"y test:\", y_test.shape)\n",
        "print(\"Input tensor shape:\", input_shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X train: (34, 784)\n",
            "y train: (34,)\n",
            "X val: (10, 784)\n",
            "y val: (10,)\n",
            "X test: (10, 784)\n",
            "y test: (10,)\n",
            "Input tensor shape: (784,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2D1xsis98pW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "275b915d-0e79-4b06-abe1-74a7d5403dc1"
      },
      "source": [
        "### Convert labels (integers) to one-hot encoding\n",
        "\n",
        "# Get number of classes\n",
        "num_classes = len(labels)\n",
        "\n",
        "# Use Keras's np_utils to create one-hot encoding (note the capital 'Y' - 2D array)\n",
        "Y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "Y_val = np_utils.to_categorical(y_val, num_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# Print out shapes (capital 'Y' is our one-hot matrix!)\n",
        "print(\"Y train:\", Y_train.shape)\n",
        "print(\"Y val:\", Y_val.shape)\n",
        "print(\"Y test:\", Y_test.shape)\n",
        "\n",
        "# Print out a few examples from training set\n",
        "for i in range(10):\n",
        "  print(\"Label: \" + str(y_train[i]) + \" | One-hot:\", Y_train[i])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y train: (34, 1)\n",
            "Y val: (10, 1)\n",
            "Y test: (10, 1)\n",
            "Label: 0 | One-hot: [1.]\n",
            "Label: 0 | One-hot: [1.]\n",
            "Label: 0 | One-hot: [1.]\n",
            "Label: 0 | One-hot: [1.]\n",
            "Label: 0 | One-hot: [1.]\n",
            "Label: 0 | One-hot: [1.]\n",
            "Label: 0 | One-hot: [1.]\n",
            "Label: 0 | One-hot: [1.]\n",
            "Label: 0 | One-hot: [1.]\n",
            "Label: 0 | One-hot: [1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJK-b2ds94w4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5122a50f-0f71-439c-f3e2-1ecf244e18e7"
      },
      "source": [
        "### Construct Keras model\n",
        "\n",
        "# A sequential model is a linear stack of layers\n",
        "model = Sequential()\n",
        "\n",
        "# First layer: fully-connected with relu activation\n",
        "model.add(Dense(64, input_shape=input_shape))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Second layer: fully-connected with relu activation\n",
        "model.add(Dense(64))   # Input shape is determined automatically from previous layer\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Third layer: 10 nodes, one for each class, and softmax activation\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# Configure the model's training settings\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "\n",
        "# Print out model summary\n",
        "print(model.summary())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 64)                50240     \n",
            "                                                                 \n",
            " activation (Activation)     (None, 64)                0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 64)                0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 1)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 54,465\n",
            "Trainable params: 54,465\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Rc0O_86pmwb",
        "outputId": "8dd95219-56d9-4fdc-8cf4-8748dcfc9fb9"
      },
      "source": [
        "### Train the model\n",
        "history = model.fit(X_train, \n",
        "                    Y_train, \n",
        "                    batch_size=32, \n",
        "                    epochs=200, \n",
        "                    verbose=1,\n",
        "                    validation_data=(X_val, Y_val))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "2/2 [==============================] - 1s 159ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 112/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 113/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 114/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 115/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 116/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 117/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 118/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 119/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 120/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 121/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 122/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 123/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 124/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 125/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 126/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 127/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 128/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 129/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 130/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 131/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 132/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 133/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 134/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 135/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 136/200\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 137/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 138/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 139/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 140/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 141/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 142/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 143/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 144/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 145/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 146/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 147/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 148/200\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 149/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 150/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 151/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 152/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 153/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 154/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 155/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 156/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 157/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 158/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 159/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 160/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 161/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 162/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 163/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 164/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 165/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 166/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 167/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 168/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 169/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 170/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 171/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 172/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 173/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 174/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 175/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 176/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 177/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 178/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 179/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 180/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 181/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 182/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 183/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 184/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 185/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 186/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 187/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 188/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 189/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 190/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 191/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 192/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 193/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 194/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 195/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 196/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 197/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 198/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 199/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 200/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "qJnFOLndpoN9",
        "outputId": "985db039-90a2-45ca-8479-cbb88592f9c9"
      },
      "source": [
        "### Plot training and validation accuracy and loss over time\n",
        "\n",
        "# Extract accuracy and loss values (in list form) from the history\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Create a list of epoch numbers\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# Plot training and validation loss values over time\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, color='blue', marker='.', label='Training loss')\n",
        "plt.plot(epochs, val_loss, color='orange', marker='.', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot training and validation accuracies over time\n",
        "plt.figure()\n",
        "plt.plot(epochs, acc, color='blue', marker='.', label='Training acc')\n",
        "plt.plot(epochs, val_acc, color='orange', marker='.', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAfJklEQVR4nO3de5gU5Z328e/NWQRFDioyGjAREcNpGCGgKEazAXVhIZqIbnR0N56T1fc1vhqTyGrcnEj04krUaFTUmKBJVkKirIlEVjceB0UUBUUzWQcRERQxiJx+7x9VM2nG6TnRM91U7s919dXVVU9V/frpnnuqnurpUURgZmbZ1aHYBZiZWdty0JuZZZyD3sws4xz0ZmYZ56A3M8s4B72ZWcY56P8OSVog6cxCty0mSdWSjm+D7YakT6TTN0n6RnPatmI/p0v6fWvrNGuM/Dn63YOk93Medgc+BLanj8+NiLvbv6rSIaka+NeIeKjA2w3gkIhYWai2kgYCfwY6R8S2QtRp1phOxS7AmicietRONxZqkjo5PKxU+P1YGjx0s5uTNFFSjaT/J+lN4HZJ+0j6naS1kt5Jp8ty1lkk6V/T6UpJ/yNpVtr2z5Imt7LtIEmPSNoo6SFJP5b0szx1N6fGayT9Kd3e7yX1zVn+RUl/kbRO0pWN9M9YSW9K6pgzb5qkpen0GEmPS3pX0mpJP5LUJc+25kj6Vs7jr6brvCHp7HptT5T0rKT3JL0uaWbO4kfS+3clvS9pXG3f5qw/XtLTkjak9+Ob2zct7Ofekm5Pn8M7kublLJsqaUn6HF6VNCmdv9MwmaSZta+zpIHpENa/SPpf4I/p/F+mr8OG9D1yeM76e0j6Qfp6bkjfY3tIul/Sl+s9n6WSpjX0XC0/B3027A/0Bj4GnEPyut6ePj4I+AD4USPrjwVWAH2B7wG3SlIr2v4ceAroA8wEvtjIPptT42nAWcC+QBfgUgBJQ4Eb0+0fkO6vjAZExJPAX4FP19vuz9Pp7cAl6fMZBxwHXNBI3aQ1TErr+QxwCFD/+sBfgTOAXsCJwPmS/ilddnR63ysiekTE4/W23Ru4H5idPrcfAvdL6lPvOXykbxrQVD/fRTIUeHi6revSGsYAdwJfTZ/D0UB1vv5owDHAYcBn08cLSPppX+AZIHeocRYwGhhP8j6+DNgB3AH8c20jSSOAASR9Yy0REb7tZjeSH7jj0+mJwBagWyPtRwLv5DxeRDL0A1AJrMxZ1h0IYP+WtCUJkW1A95zlPwN+1szn1FCNX895fAHwX+n0N4G5Ocv2TPvg+Dzb/hZwWzrdkySEP5an7cXAfTmPA/hEOj0H+FY6fRvwnZx2g3PbNrDd64Hr0umBadtOOcsrgf9Jp78IPFVv/ceByqb6piX9DPQnCdR9Gmj3k9p6G3v/pY9n1r7OOc/t4EZq6JW22ZvkF9EHwIgG2nUD3iG57gHJL4Qb2vvnLQs3H9Fnw9qI2Fz7QFJ3ST9JT4XfIxkq6JU7fFHPm7UTEbEpnezRwrYHAOtz5gG8nq/gZtb4Zs70ppyaDsjddkT8FViXb18kR+/TJXUFpgPPRMRf0joGp8MZb6Z1/AfJ0X1TdqoB+Eu95zdW0sPpkMkG4Lxmbrd223+pN+8vJEeztfL1zU6a6OcDSV6zdxpY9UDg1WbW25C6vpHUUdJ30uGf9/jbmUHf9NatoX2l7+l7gH+W1AGYQXIGYi3koM+G+h+d+r/AocDYiNiLvw0V5BuOKYTVQG9J3XPmHdhI+12pcXXuttN99snXOCJeJAnKyew8bAPJENBykqPGvYCvtaYGkjOaXD8H5gMHRsTewE05223qo25vkAy15DoIWNWMuuprrJ9fJ3nNejWw3uvAx/Ns868kZ3O19m+gTe5zPA2YSjK8tTfJUX9tDW8DmxvZ1x3A6SRDapui3jCXNY+DPpt6kpwOv5uO917V1jtMj5CrgJmSukgaB/xjG9X4K+AkSUelF06vpun38s+BfyMJul/Wq+M94H1JQ4Dzm1nDvUClpKHpL5r69fckOVrenI53n5azbC3JkMnBebb9ADBY0mmSOkn6AjAU+F0za6tfR4P9HBGrScbOb0gv2naWVPuL4FbgLEnHSeogaUDaPwBLgFPT9hXAyc2o4UOSs67uJGdNtTXsIBkG+6GkA9Kj/3Hp2RdpsO8AfoCP5lvNQZ9N1wN7kBwtPQH8Vzvt93SSC5rrSMbF7yH5AW9Iq2uMiGXAhSThvZpkHLemidV+QXKB8I8R8XbO/EtJQngjcEtac3NqWJA+hz8CK9P7XBcAV0vaSHJN4d6cdTcB1wJ/UvJpn0/V2/Y64CSSo/F1JBcnT6pXd3M11c9fBLaSnNW8RXKNgoh4iuRi73XABuC/+dtZxjdIjsDfAf6dnc+QGnInyRnVKuDFtI5clwLPA08D64HvsnM23QkMI7nmY63gP5iyNiPpHmB5RLT5GYVll6QzgHMi4qhi17K78hG9FYykIyR9PD3Vn0QyLjuvqfXM8kmHxS4Abi52LbszB70V0v4kH/17n+Qz4OdHxLNFrch2W5I+S3I9Yw1NDw9ZIzx0Y2aWcT6iNzPLuJL7UrO+ffvGwIEDi12GmdluZfHixW9HRL+GlpVc0A8cOJCqqqpil2FmtluRVP+vqet46MbMLOMc9GZmGeegNzPLuJIbozez4tm6dSs1NTVs3ry56cZWFN26daOsrIzOnTs3ex0HvZnVqampoWfPngwcOJD8/3vGiiUiWLduHTU1NQwaNKjZ63noxszqbN68mT59+jjkS5Qk+vTp0+IzLge9me3EIV/aWvP6OOjNzDLOQW9mJWPdunWMHDmSkSNHsv/++zNgwIC6x1u2bGl03aqqKr7yla80uY/x48cXqtzdhi/GmlnJ6NOnD0uWLAFg5syZ9OjRg0svvbRu+bZt2+jUqeHYqqiooKKiosl9PPbYY4UpdjfiI3oz2yWPPw7f/nZy3xYqKys577zzGDt2LJdddhlPPfUU48aNY9SoUYwfP54VK1YAsGjRIk466SQg+SVx9tlnM3HiRA4++GBmz55dt70ePXrUtZ84cSInn3wyQ4YM4fTTT6f223wfeOABhgwZwujRo/nKV75St91c1dXVTJgwgfLycsrLy3f6BfLd736XYcOGMWLECC6//HIAVq5cyfHHH8+IESMoLy/n1Vd35X+vt4yP6M2sQRdfDOnBdV4bNsDSpbBjB3ToAMOHw957528/ciRcf33La6mpqeGxxx6jY8eOvPfeezz66KN06tSJhx56iK997Wv8+te//sg6y5cv5+GHH2bjxo0ceuihnH/++R/57Pmzzz7LsmXLOOCAAzjyyCP505/+REVFBeeeey6PPPIIgwYNYsaMGQ3WtO+++/KHP/yBbt268corrzBjxgyqqqpYsGABv/nNb3jyySfp3r0769evB+D000/n8ssvZ9q0aWzevJkdO3a0vCNayUFvZq22YUMS8pDcb9jQeNC31imnnELHjh3TfW7gzDPP5JVXXkESW7dubXCdE088ka5du9K1a1f23Xdf1qxZQ1lZ2U5txowZUzdv5MiRVFdX06NHDw4++OC6z6nPmDGDm2/+6D+42rp1KxdddBFLliyhY8eOvPzyywA89NBDnHXWWXTv3h2A3r17s3HjRlatWsW0adOA5I+e2pOD3swa1Jwj78cfh+OOgy1boEsXuPtuGDeu8LXsueeeddPf+MY3OPbYY7nvvvuorq5m4sSJDa7TtWvXuumOHTuybdu2VrXJ57rrrmO//fbjueeeY8eOHe0e3i3hMXoza7Vx42DhQrjmmuS+LUK+vg0bNjBgwAAA5syZU/DtH3roobz22mtUV1cDcM899+Sto3///nTo0IG77rqL7du3A/CZz3yG22+/nU2bNgGwfv16evbsSVlZGfPmJf9C+cMPP6xb3h4c9Ga2S8aNgyuuaJ+QB7jsssu44oorGDVqVIuOwJtrjz324IYbbmDSpEmMHj2anj17sncD41EXXHABd9xxByNGjGD58uV1Zx2TJk1iypQpVFRUMHLkSGbNmgXAXXfdxezZsxk+fDjjx4/nzTffLHjt+ZTc/4ytqKgI/+MRs+J46aWXOOyww4pdRtG9//779OjRg4jgwgsv5JBDDuGSSy4pdll1GnqdJC2OiAY/X+ojejOzem655RZGjhzJ4YcfzoYNGzj33HOLXdIu8cVYM7N6LrnkkpI6gt9VPqI3M8s4B72ZWcY56M3MMs5Bb2aWcQ56MysZxx57LA8++OBO866//nrOP//8vOtMnDiR2o9kn3DCCbz77rsfaTNz5sy6z7PnM2/ePF588cW6x9/85jd56KGHWlJ+yXLQm1nJmDFjBnPnzt1p3ty5c/N+sVh9DzzwAL169WrVvusH/dVXX83xxx/fqm2VmiaDXtJtkt6S9EKe5ZI0W9JKSUsllddbvpekGkk/KlTRZlZC1j4Oy76d3O+ik08+mfvvv7/un4xUV1fzxhtvMGHCBM4//3wqKio4/PDDueqqqxpcf+DAgbz99tsAXHvttQwePJijjjqq7quMIfmM/BFHHMGIESP43Oc+x6ZNm3jssceYP38+X/3qVxk5ciSvvvoqlZWV/OpXvwJg4cKFjBo1imHDhnH22Wfz4Ycf1u3vqquuory8nGHDhrF8+fKP1FQKX2fcnM/RzwF+BNyZZ/lk4JD0Nha4Mb2vdQ3wSOtLNLOiWHwxvNPE9xRv3QDvLAV2AB1gn+HQuZGvr9xnJIzO/21pvXv3ZsyYMSxYsICpU6cyd+5cPv/5zyOJa6+9lt69e7N9+3aOO+44li5dyvDhwxsuffFi5s6dy5IlS9i2bRvl5eWMHj0agOnTp/OlL30JgK9//evceuutfPnLX2bKlCmcdNJJnHzyyTtta/PmzVRWVrJw4UIGDx7MGWecwY033sjFF18MQN++fXnmmWe44YYbmDVrFj/96U93Wr8Uvs64ySP6iHgEWN9Ik6nAnZF4AuglqT+ApNHAfsDvd7lSMys9WzaQhDzJ/ZYNu7zJ3OGb3GGbe++9l/LyckaNGsWyZct2Gmap79FHH2XatGl0796dvfbaiylTptQte+GFF5gwYQLDhg3j7rvvZtmyZY3Ws2LFCgYNGsTgwYMBOPPMM3nkkb8du06fPh2A0aNH130RWq6tW7fypS99iWHDhnHKKafU1d3crzOuXb4rCvGXsQOA13Me1wADJK0BfgD8M9DoQJekc4BzAA466KAClGRmu6yRI+86ax+HPx4HO7ZAhy4w/m7ot2vfbjZ16lQuueQSnnnmGTZt2sTo0aP585//zKxZs3j66afZZ599qKysZPPmza3afmVlJfPmzWPEiBHMmTOHRYsW7VK9tV91nO9rjkvh64zb8mLsBcADEVHTVMOIuDkiKiKiol+/fm1YkpkVVL9x8OmFMPya5H4XQx6Sf/V37LHHcvbZZ9cdzb/33nvsueee7L333qxZs4YFCxY0uo2jjz6aefPm8cEHH7Bx40Z++9vf1i3buHEj/fv3Z+vWrdx9991183v27MnGjRs/sq1DDz2U6upqVq5cCSTfQnnMMcc0+/mUwtcZFyLoVwEH5jwuS+eNAy6SVA3MAs6Q9J0C7M/MSkm/cXD4FQUJ+VozZszgueeeqwv6ESNGMGrUKIYMGcJpp53GkUce2ej65eXlfOELX2DEiBFMnjyZI444om7ZNddcw9ixYznyyCMZMmRI3fxTTz2V73//+4waNWqnC6DdunXj9ttv55RTTmHYsGF06NCB8847r9nPpRS+zrhZX1MsaSDwu4j4ZAPLTgQuAk4guQg7OyLG1GtTCVRExEVN7ctfU2xWPP6a4t1DS7+muMkxekm/ACYCfSXVAFcBnQEi4ibgAZKQXwlsAs7ahfrNzKzAmgz6iGj0LxUiOSW4sIk2c0g+pmlmZu3MfxlrZjsptf86ZztrzevjoDezOt26dWPdunUO+xIVEaxbt67FH9H0f5gyszplZWXU1NSwdu3aYpdieXTr1o2ysrIWreOgN7M6nTt3ZtCgQcUuwwrMQzdmZhnnoDczyzgHvZlZxjnozcwyzkFvZpZxDnozs4xz0JuZZZyD3sws4xz0ZmYZ56A3M8s4B72ZWcY56M3MMs5Bb2aWcQ56M7OMc9CbmWWcg97MLOMc9GZmGeegNzPLOAe9mVnGOejNzDLOQW9mlnEOejOzjHPQm5llnIPezCzjHPRmZhnnoDczy7gmg17SbZLekvRCnuWSNFvSSklLJZWn80dKelzSsnT+FwpdvJmZNa05R/RzgEmNLJ8MHJLezgFuTOdvAs6IiMPT9a+X1Kv1pZqZWWt0aqpBRDwiaWAjTaYCd0ZEAE9I6iWpf0S8nLONNyS9BfQD3t3Fms3MrAUKMUY/AHg953FNOq+OpDFAF+DVAuzPzMxaoM0vxkrqD9wFnBURO/K0OUdSlaSqtWvXtnVJZmZ/VwoR9KuAA3Mel6XzkLQXcD9wZUQ8kW8DEXFzRFREREW/fv0KUJKZmdUqRNDPB85IP33zKWBDRKyW1AW4j2T8/lcF2I+ZmbVCkxdjJf0CmAj0lVQDXAV0BoiIm4AHgBOAlSSftDkrXfXzwNFAH0mV6bzKiFhSwPrNzKwJzfnUzYwmlgdwYQPzfwb8rPWlmZlZIfgvY83MMs5Bb2aWcQ56M7OMc9CbmWWcg97MLOMc9GZmGeegNzPLOAe9mVnGOejNzDLOQW9mlnEOejOzjHPQm5llnIPezCzjHPRmZhnnoDczyzgHvZlZxjnozcwyzkFvZpZxDnozs4xz0JuZZZyD3sws4xz0ZmYZ56A3M8s4B72ZWcY56M3MMs5Bb2aWcQ56M7OMc9CbmWWcg97MLOMc9GZmGeegNzPLuCaDXtJtkt6S9EKe5ZI0W9JKSUsllecsO1PSK+ntzEIWbmZmzdOpGW3mAD8C7syzfDJwSHobC9wIjJXUG7gKqAACWCxpfkS8s6tF5/P8w4+zfnFaZu9RsP7ZVk/HB+vQHn12aRvej/fj/exe2y6F/fQefQbDjh1HISkimm4kDQR+FxGfbGDZT4BFEfGL9PEKYGLtLSLObahdPhUVFVFVVdWiJwFJyB9WM4GOHba3eF0zs1Lx4bauvHLQwy0Oe0mLI6KioWWFGKMfALye87gmnZdvfkMFniOpSlLV2rVrW1XEuhcX0UHbkUCq3e6uTRdiG96P9+P97F7bLvZ+OnfcwroXF1FIJXExNiJujoiKiKjo169fq7bRZ+hEtmzrQgTUnqTsyvS27drlbXg/3o/3s3ttuxT2s3V7F/oMnUghNWeMvimrgANzHpel81aRDN/kzl9UgP01aNix43j+4UUeo/d+vJ+M7ycLz6G9x+gLEfTzgYskzSW5GLshIlZLehD4D0n7pO3+AbiiAPvLa9ix46DAHWRmtrtrMugl/YLkyLyvpBqST9J0BoiIm4AHgBOAlcAm4Kx02XpJ1wBPp5u6OiLWF/oJmJlZ45oM+oiY0cTyAC7Ms+w24LbWlWZmZoVQEhdjzcys7TjozcwyzkFvZpZxDnozs4xz0JuZZZyD3sws4xz0ZmYZ56A3M8s4B72ZWcY56M3MMs5Bb2aWcQ56M7OMc9CbmWWcg97MLOMc9GZmGeegNzPLOAe9mVnGOejNzDLOQW9mlnEOejOzjHPQm5llnIPezCzjHPRmZhnnoDczyzgHvZlZxjnozcwyzkFvZpZxDnozs4xz0JuZZZyD3sws45oV9JImSVohaaWkyxtY/jFJCyUtlbRIUlnOsu9JWibpJUmzJamQT8DMzBrXZNBL6gj8GJgMDAVmSBpar9ks4M6IGA5cDXw7XXc8cCQwHPgkcARwTMGqNzOzJjXniH4MsDIiXouILcBcYGq9NkOBP6bTD+csD6Ab0AXoCnQG1uxq0WZm1nzNCfoBwOs5j2vSebmeA6an09OAnpL6RMTjJMG/Or09GBEv7VrJZmbWEoW6GHspcIykZ0mGZlYB2yV9AjgMKCP55fBpSRPqryzpHElVkqrWrl1boJLMzAyaF/SrgANzHpel8+pExBsRMT0iRgFXpvPeJTm6fyIi3o+I94EFwLj6O4iImyOiIiIq+vXr18qnYmZmDWlO0D8NHCJpkKQuwKnA/NwGkvpKqt3WFcBt6fT/khzpd5LUmeRo30M3ZmbtqMmgj4htwEXAgyQhfW9ELJN0taQpabOJwApJLwP7Adem838FvAo8TzKO/1xE/LawT8HMzBqjiCh2DTupqKiIqqqqYpdhZrZbkbQ4IioaWua/jDUzyzgHvZlZxjnozcwyzkFvZpZxDnozs4xz0JuZZZyD3sws4xz0ZmYZ56A3M8s4B72ZWcY56M3MMs5Bb2aWcQ56M7OMc9CbmWWcg97MLOMc9GZmGeegNzPLOAe9mVnGOejNzDLOQW9mlnEOejOzjHPQm5llnIPezCzjHPRmZhnnoDczyzgHvZlZxjnozcwyzkFvZpZxDnozs4xz0JuZZZyD3sws45oV9JImSVohaaWkyxtY/jFJCyUtlbRIUlnOsoMk/V7SS5JelDSwcOWbmVlTmgx6SR2BHwOTgaHADElD6zWbBdwZEcOBq4Fv5yy7E/h+RBwGjAHeKkThZmbWPM05oh8DrIyI1yJiCzAXmFqvzVDgj+n0w7XL018InSLiDwAR8X5EbCpI5WZm1izNCfoBwOs5j2vSebmeA6an09OAnpL6AIOBdyX9p6RnJX0/PUPYiaRzJFVJqlq7dm3Ln4WZmeVVqIuxlwLHSHoWOAZYBWwHOgET0uVHAAcDlfVXjoibI6IiIir69etXoJLMzAyaF/SrgANzHpel8+pExBsRMT0iRgFXpvPeJTn6X5IO+2wD5gHlBanczMyapTlB/zRwiKRBkroApwLzcxtI6iupdltXALflrNtLUu1h+qeBF3e9bDMza64mgz49Er8IeBB4Cbg3IpZJulrSlLTZRGCFpJeB/YBr03W3kwzbLJT0PCDgloI/CzMzy0sRUewadlJRURFVVVXFLsPMbLciaXFEVDS0zH8Za2aWcQ56M7OMc9CbmWWcg97MLOMc9GZmGeegNzPLOAe9mVnGOejNzDLOQW9mlnEOejOzjHPQm5llnIPezCzjHPRmZhnnoDczyzgHvZlZxjnozcwyzkFvZpZxDnozs4xz0JuZZZyD3sws4xz0ZmYZ56A3M8s4B72ZWcY56M3MMk4RUewadiJpLfCXVqzaF3i7wOUUQqnWBaVbm+tqmVKtC0q3tizW9bGI6NfQgpIL+taSVBURFcWuo75SrQtKtzbX1TKlWheUbm1/b3V56MbMLOMc9GZmGZeloL+52AXkUap1QenW5rpaplTrgtKt7e+qrsyM0ZuZWcOydERvZmYNcNCbmWVcJoJe0iRJKyStlHR5Ees4UNLDkl6UtEzSv6XzZ0paJWlJejuhCLVVS3o+3X9VOq+3pD9IeiW936edazo0p0+WSHpP0sXF6i9Jt0l6S9ILOfMa7CMlZqfvuaWSytu5ru9LWp7u+z5JvdL5AyV9kNN3N7VzXXlfO0lXpP21QtJn27mue3Jqqpa0JJ3fnv2VLx/a/j0WEbv1DegIvAocDHQBngOGFqmW/kB5Ot0TeBkYCswELi1yP1UDfevN+x5weTp9OfDdIr+ObwIfK1Z/AUcD5cALTfURcAKwABDwKeDJdq7rH4BO6fR3c+oamNuuCP3V4GuX/hw8B3QFBqU/sx3bq656y38AfLMI/ZUvH9r8PZaFI/oxwMqIeC0itgBzganFKCQiVkfEM+n0RuAlYEAxammmqcAd6fQdwD8VsZbjgFcjojV/FV0QEfEIsL7e7Hx9NBW4MxJPAL0k9W+vuiLi9xGxLX34BFDWFvtuaV2NmArMjYgPI+LPwEqSn912rUuSgM8Dv2iLfTemkXxo8/dYFoJ+APB6zuMaSiBcJQ0ERgFPprMuSk+/bmvvIZJUAL+XtFjSOem8/SJidTr9JrBfEeqqdSo7//AVu79q5eujUnrfnU1y5FdrkKRnJf23pAlFqKeh165U+msCsCYiXsmZ1+79VS8f2vw9loWgLzmSegC/Bi6OiPeAG4GPAyOB1SSnju3tqIgoByYDF0o6OndhJOeKRfmsraQuwBTgl+msUuivjyhmH+Uj6UpgG3B3Oms1cFBEjAL+D/BzSXu1Y0kl+drlmMHOBxTt3l8N5EOdtnqPZSHoVwEH5jwuS+cVhaTOJC/i3RHxnwARsSYitkfEDuAW2uiUtTERsSq9fwu4L61hTe2pYHr/VnvXlZoMPBMRa9Iai95fOfL1UdHfd5IqgZOA09OAIB0aWZdOLyYZCx/cXjU18tqVQn91AqYD99TOa+/+aigfaIf3WBaC/mngEEmD0iPDU4H5xSgkHf+7FXgpIn6YMz93XG0a8EL9ddu4rj0l9aydJrmQ9wJJP52ZNjsT+E171pVjp6OsYvdXPfn6aD5wRvrJiE8BG3JOv9ucpEnAZcCUiNiUM7+fpI7p9MHAIcBr7VhXvtduPnCqpK6SBqV1PdVedaWOB5ZHRE3tjPbsr3z5QHu8x9rjanNb30iuTr9M8tv4yiLWcRTJaddSYEl6OwG4C3g+nT8f6N/OdR1M8omH54BltX0E9AEWAq8ADwG9i9BnewLrgL1z5hWlv0h+2awGtpKMh/5Lvj4i+STEj9P33PNARTvXtZJk/Lb2fXZT2vZz6Wu8BHgG+Md2rivvawdcmfbXCmBye9aVzp8DnFevbXv2V758aPP3mL8Cwcws47IwdGNmZo1w0JuZZZyD3sws4xz0ZmYZ56A3M8s4B72ZWcY56M3MMu7/A4FqTwPFCu1wAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgmX9gfJpq-L",
        "outputId": "b04335ad-9335-4e62-d5b9-76f6923b8ea0"
      },
      "source": [
        "### Try predicting label with one validation sample (inference)\n",
        "\n",
        "# Change this to try a different sample from the test set\n",
        "idx = 0\n",
        "\n",
        "# Make sample 2D array instead of 1D vector (this is what the Keras model expects as input)\n",
        "x = np.expand_dims(X_val[idx], 0)\n",
        "\n",
        "# Make prediction using trained model\n",
        "y_pred = model.predict(x)\n",
        "\n",
        "# Find index of highest score in output\n",
        "predicted_label = np.argmax(y_pred)\n",
        "actual_label = np.argmax(Y_val[idx])\n",
        "\n",
        "# Display model output, predicted label, actual label\n",
        "print(\"Model output:\", y_pred)\n",
        "print(\"Predicted label:\", predicted_label, \"-\", labels[predicted_label])\n",
        "print(\"Actual label:\", actual_label, \"-\", labels[actual_label])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model output: [[1.]]\n",
            "Predicted label: 0 - image\n",
            "Actual label: 0 - image\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Lo3KWpjptJk",
        "outputId": "d54f5b44-df89-46c4-bcba-0ec12025eb15"
      },
      "source": [
        "### Create confusion matrix from validation set\n",
        "\n",
        "# Find predictions from all validation samples\n",
        "Y_pred = model.predict(X_val)\n",
        "print(\"Validation output shape:\", Y_pred.shape)\n",
        "\n",
        "# Convert actual and predicted validation one-hot encoding to numerical labels\n",
        "y_val = np.argmax(Y_val, axis=1)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "# Print some values from actual and predicted validation sets (first 50 samples)\n",
        "print(\"Actual validation labels:\\t\", y_val[:50])\n",
        "print(\"Predicted validation labels:\\t\", y_pred[:50])\n",
        "\n",
        "# Compute confusion matrix (note: we need to transpose SKLearn matrix to make it match Edge Impulse)\n",
        "cm = confusion_matrix(y_val, y_pred)\n",
        "cm = np.transpose(cm)\n",
        "\n",
        "# Print confusion matrix\n",
        "print()\n",
        "print(\" ---> Predicted labels\")\n",
        "print(\"|\")\n",
        "print(\"v Actual labels\")\n",
        "print(\"\\t\\t\\t\" + ' '.join(\"{!s:6}\".format('(' + str(i) + ')') for i in range(num_classes)))\n",
        "for row in range(num_classes):\n",
        "  print(\"{:>12} ({}):  [{}]\".format(labels[row], row, ' '.join(\"{:6}\".format(i) for i in cm[row])))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation output shape: (10, 1)\n",
            "Actual validation labels:\t [0 0 0 0 0 0 0 0 0 0]\n",
            "Predicted validation labels:\t [0 0 0 0 0 0 0 0 0 0]\n",
            "\n",
            " ---> Predicted labels\n",
            "|\n",
            "v Actual labels\n",
            "\t\t\t(0)   \n",
            "       image (0):  [    10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgZXa_y2qorZ",
        "outputId": "19ab9cc3-f180-4125-e504-aae2d539e600"
      },
      "source": [
        "### Evaluate model on validation set\n",
        "score = model.evaluate(X_val, Y_val)\n",
        "print(\"Validation loss:\", score[0])\n",
        "print(\"Validation accuracy:\", score[1])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - acc: 1.0000\n",
            "Validation loss: 0.0\n",
            "Validation accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2lvFg0RVxFr",
        "outputId": "5add6916-2d18-4bea-f8d7-b5b4a4b5d60c"
      },
      "source": [
        "### Evaluate model on entire test set\n",
        "score = model.evaluate(X_test, Y_test)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - acc: 1.0000\n",
            "Test loss: 0.0\n",
            "Test accuracy: 1.0\n"
          ]
        }
      ]
    }
  ]
}